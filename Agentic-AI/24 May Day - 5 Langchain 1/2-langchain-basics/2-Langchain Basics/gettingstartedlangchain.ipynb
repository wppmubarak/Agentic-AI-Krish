{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000278244FE8B0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000278244FEC40> root_client=<openai.OpenAI object at 0x00000278244FE650> root_async_client=<openai.AsyncOpenAI object at 0x00000278244FE9E0> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "#llm=ChatOpenAI(model=\"qwen-qwq-32b\")\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems designed to function as autonomous agents capable of making decisions, taking actions, and achieving specific goals without continuous human intervention. The concept centers around granting AI entities a degree of **agency**, meaning they can operate independently within their defined parameters to perform tasks, adapt to changing environments, and improve their performance over time through learning.\\n\\n### Key Characteristics of Agentic AI\\n\\n1. **Autonomy**: Ability to operate independently without requiring constant human oversight.\\n2. **Goal-Oriented Behavior**: Possessing specific objectives that guide decision-making and actions.\\n3. **Perception and Understanding**: Utilizing sensors and data inputs to interpret and understand the environment.\\n4. **Decision-Making Capability**: Evaluating options and selecting the most appropriate actions to achieve set goals.\\n5. **Learning and Adaptation**: Continuously improving performance by learning from experiences and adapting to new information or changes in the environment.\\n\\n### Examples of Agentic AI\\n\\n- **Autonomous Vehicles**: Self-driving cars that navigate roads, avoid obstacles, and make real-time driving decisions.\\n- **Virtual Assistants**: AI like Siri, Alexa, or Google Assistant that can manage schedules, answer queries, and control smart home devices autonomously.\\n- **Robotic Process Automation (RPA)**: Software robots that handle repetitive tasks such as data entry, transaction processing, or customer service interactions without human intervention.\\n- **Intelligent Agents in Gaming**: Non-player characters (NPCs) that adapt their behavior based on player actions to provide a more dynamic gaming experience.\\n\\n### Applications and Implications\\n\\n- **Efficiency and Productivity**: Agentic AI can handle routine or complex tasks more efficiently, freeing up human resources for more strategic activities.\\n- **Personalization**: By learning from interactions, agentic AI can offer personalized experiences in areas like healthcare, education, and entertainment.\\n- **Decision Support**: Assisting professionals in making informed decisions by processing vast amounts of data and identifying patterns that may not be immediately apparent.\\n\\n### Ethical and Societal Considerations\\n\\nThe development and deployment of agentic AI raise several ethical and societal issues, including:\\n\\n- **Accountability**: Determining responsibility for the actions taken by autonomous AI systems.\\n- **Bias and Fairness**: Ensuring that AI decision-making processes are free from biases that could lead to unfair treatment of individuals or groups.\\n- **Privacy**: Protecting sensitive data that AI systems may access and process.\\n- **Employment Impact**: Addressing the potential displacement of jobs due to automation and finding ways to retrain affected workers.\\n\\n### Future Outlook\\n\\nAgentic AI continues to advance, with ongoing research focused on enhancing autonomy, improving decision-making algorithms, and ensuring ethical standards are met. As these systems become more integrated into various aspects of daily life and industry, it is crucial to balance innovation with responsible development to maximize benefits while mitigating risks.\\n\\n---\\n\\n**In Summary**, Agentic AI embodies the next generation of artificial intelligence systems that operate with a higher degree of independence and capability. By enabling machines to set and achieve goals autonomously, Agentic AI promises to transform industries, enhance personal experiences, and address complex challenges, all while necessitating careful consideration of ethical and societal impacts.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1561, 'prompt_tokens': 13, 'total_tokens': 1574, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 896, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o1-mini-2024-09-12', 'system_fingerprint': 'fp_3da8b0b088', 'id': 'chatcmpl-BpSQ2izUEOOJIRFuSgWl1YSgP66Bz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ce005d39-1112-493f-a2a7-7a0a3c3259a3-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1561, 'total_tokens': 1574, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 896}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems designed to function as autonomous agents capable of making decisions, taking actions, and achieving specific goals without continuous human intervention. The concept centers around granting AI entities a degree of **agency**, meaning they can operate independently within their defined parameters to perform tasks, adapt to changing environments, and improve their performance over time through learning.\n",
      "\n",
      "### Key Characteristics of Agentic AI\n",
      "\n",
      "1. **Autonomy**: Ability to operate independently without requiring constant human oversight.\n",
      "2. **Goal-Oriented Behavior**: Possessing specific objectives that guide decision-making and actions.\n",
      "3. **Perception and Understanding**: Utilizing sensors and data inputs to interpret and understand the environment.\n",
      "4. **Decision-Making Capability**: Evaluating options and selecting the most appropriate actions to achieve set goals.\n",
      "5. **Learning and Adaptation**: Continuously improving performance by learning from experiences and adapting to new information or changes in the environment.\n",
      "\n",
      "### Examples of Agentic AI\n",
      "\n",
      "- **Autonomous Vehicles**: Self-driving cars that navigate roads, avoid obstacles, and make real-time driving decisions.\n",
      "- **Virtual Assistants**: AI like Siri, Alexa, or Google Assistant that can manage schedules, answer queries, and control smart home devices autonomously.\n",
      "- **Robotic Process Automation (RPA)**: Software robots that handle repetitive tasks such as data entry, transaction processing, or customer service interactions without human intervention.\n",
      "- **Intelligent Agents in Gaming**: Non-player characters (NPCs) that adapt their behavior based on player actions to provide a more dynamic gaming experience.\n",
      "\n",
      "### Applications and Implications\n",
      "\n",
      "- **Efficiency and Productivity**: Agentic AI can handle routine or complex tasks more efficiently, freeing up human resources for more strategic activities.\n",
      "- **Personalization**: By learning from interactions, agentic AI can offer personalized experiences in areas like healthcare, education, and entertainment.\n",
      "- **Decision Support**: Assisting professionals in making informed decisions by processing vast amounts of data and identifying patterns that may not be immediately apparent.\n",
      "\n",
      "### Ethical and Societal Considerations\n",
      "\n",
      "The development and deployment of agentic AI raise several ethical and societal issues, including:\n",
      "\n",
      "- **Accountability**: Determining responsibility for the actions taken by autonomous AI systems.\n",
      "- **Bias and Fairness**: Ensuring that AI decision-making processes are free from biases that could lead to unfair treatment of individuals or groups.\n",
      "- **Privacy**: Protecting sensitive data that AI systems may access and process.\n",
      "- **Employment Impact**: Addressing the potential displacement of jobs due to automation and finding ways to retrain affected workers.\n",
      "\n",
      "### Future Outlook\n",
      "\n",
      "Agentic AI continues to advance, with ongoing research focused on enhancing autonomy, improving decision-making algorithms, and ensuring ethical standards are met. As these systems become more integrated into various aspects of daily life and industry, it is crucial to balance innovation with responsible development to maximize benefits while mitigating risks.\n",
      "\n",
      "---\n",
      "\n",
      "**In Summary**, Agentic AI embodies the next generation of artificial intelligence systems that operate with a higher degree of independence and capability. By enabling machines to set and achieve goals autonomously, Agentic AI promises to transform industries, enhance personal experiences, and address complex challenges, all while necessitating careful consideration of ethical and societal impacts.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said, \"Hi My name is Mubarak.\" I need to respond appropriately. Let me start by greeting them back. I should mention their name to make it personal. Maybe say something like, \"Hello Mubarak! How can I assist you today?\" That\\'s friendly and opens the conversation. Wait, should I add more? Maybe ask how they\\'re doing too? \"Hello Mubarak! How are you today and how can I assist you?\" Hmm, that might be a bit long. Let me stick to the first idea. Keep it simple and welcoming. I should check for any cultural considerations. Since the name Mubarak is common in some regions, but I don\\'t need to overcomplicate. Just a standard greeting should be fine. Alright, I\\'ll go with the first response.\\n</think>\\n\\nHello Mubarak! How can I assist you today? I\\'m here to help with any questions or tasks you might have. 😊', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 199, 'prompt_tokens': 17, 'total_tokens': 216, 'completion_time': 0.461671937, 'prompt_time': 0.003146335, 'queue_time': 0.265255033, 'total_time': 0.464818272}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--e4ac74f7-aea4-4eec-8366-ddcaaf61b24e-0', usage_metadata={'input_tokens': 17, 'output_tokens': 199, 'total_tokens': 216})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Mubarak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027825ED9310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027825ED9D10>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000027825ED9310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000027825ED9D10>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're asking about **Langsmith**, a pretty neat tool in the world of AI!  \n",
      "\n",
      "Here's what I can tell you:\n",
      "\n",
      "**Langsmith: Your AI Assistant for Fine-Tuning**\n",
      "\n",
      "Langsmith is an open-source platform designed to make fine-tuning large language models (LLMs) more accessible and user-friendly. Think of it as a specialized workshop for tweaking and customizing pre-trained LLMs like GPT to perform specific tasks better.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Simplified Interface:**  Langsmith provides a web-based interface that streamlines the fine-tuning process. No need to be a coding expert to get started!\n",
      "\n",
      "* **Dataset Management:**  It helps you organize and manage your training data efficiently.\n",
      "\n",
      "* **Experiment Tracking:** Langsmith allows you to track different fine-tuning experiments, compare results, and easily reproduce successful runs.\n",
      "\n",
      "* **Community Collaboration:** Being open-source, Langsmith fosters a community where users can share datasets, fine-tuned models, and best practices.\n",
      "\n",
      "**Why Fine-Tune?**\n",
      "\n",
      "Pre-trained LLMs are powerful, but they might not be perfect for your specific use case. Fine-tuning allows you to:\n",
      "\n",
      "* **Improve Accuracy:** Tailor the model to your specific domain or task for better performance.\n",
      "* **Reduce Bias:** Address potential biases in the original training data.\n",
      "* **Customize Behavior:**  Adjust the model's output style, tone, or content generation to match your needs.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "* **Chatbot Specialization:** Fine-tune a model to excel at customer service or technical support.\n",
      "* **Code Generation:**  Train a model to generate code in a particular programming language.\n",
      "* **Summarization:**  Optimize a model for concise and accurate text summarization.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "Langsmith is a valuable tool for anyone interested in exploring the potential of fine-tuning LLMs.  You can find more information and documentation on their website: [https://github.com/langsmithai/langsmith](https://github.com/langsmithai/langsmith)\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about Langsmith or fine-tuning in general – I'm here to help!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I'm happy to tell you about Langsmith!\n",
      "\n",
      "**Langsmith** is an open-source platform designed to simplify the process of building and deploying **language models (LLMs)**. Think of it as a toolbox specifically for working with these powerful AI systems.\n",
      "\n",
      "Here are some key things to know about Langsmith:\n",
      "\n",
      "* **Ease of Use:** Langsmith aims to make LLM development more accessible to a wider range of users, even those without extensive machine learning expertise. It provides a user-friendly interface and streamlined workflows for common tasks.\n",
      "\n",
      "* **Modular Design:** The platform is built with modular components, allowing you to easily customize and extend its functionality. You can integrate your own datasets, training algorithms, and evaluation metrics.\n",
      "\n",
      "* **Community-Driven:** Langsmith is open-source, meaning its code, documentation, and resources are freely available to the public. This fosters collaboration, innovation, and shared learning within the AI community.\n",
      "\n",
      "* **Focus on Efficiency:** Langsmith incorporates techniques to optimize LLM training and deployment, helping you save time and resources.\n",
      "\n",
      "**Here are some of the things Langsmith helps you do:**\n",
      "\n",
      "* **Fine-tune pre-trained LLMs:** Adapt existing models to your specific tasks and domains by training them on your own data.\n",
      "* **Create custom LLMs from scratch:** Build entirely new language models tailored to your unique needs.\n",
      "* **Experiment with different architectures and training methods:** Explore various LLM designs and techniques to find the best solutions for your applications.\n",
      "* **Deploy and integrate LLMs into applications:** Easily connect your trained models to real-world software systems.\n",
      "\n",
      "**Where to Learn More:**\n",
      "\n",
      "* **Official Website:** [https://www.langsmith.com/](https://www.langsmith.com/)\n",
      "* **GitHub Repository:** [https://github.com/langsmithai/langsmith](https://github.com/langsmithai/langsmith)\n",
      "\n",
      "Langsmith is a valuable resource for anyone interested in exploring and leveraging the power of language models. Its user-friendly nature, modular design, and community support make it an excellent choice for both beginners and experienced AI practitioners.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for developing and deploying large language models (LLMs).', 'features': ['Modular architecture allowing for customization and extensibility', 'Support for various LLM frameworks like Transformers and PyTorch', 'Tools for fine-tuning and evaluating LLMs', 'Deployment options for both cloud and on-premise environments', 'Community-driven development with active contributions'], 'website': 'https://github.com/langs-team/langs'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ee96082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed7d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Langsmith is an open-source platform developed by AssemblyAI that simplifies the process of building and deploying AI-powered applications, particularly those involving natural language understanding (NLU). \\n\\n  Here are some key features and aspects of Langsmith:\\n\\n  * **Low-Code/No-Code Approach:** Langsmith aims to make AI development accessible to a wider audience by providing a user-friendly interface and tools that require minimal coding knowledge.\\n  * **Model Hub:** It offers a curated library of pre-trained language models from various sources, allowing users to easily integrate powerful AI capabilities into their projects.\\n  * **Fine-Tuning:** Langsmith enables users to fine-tune these pre-trained models on their own datasets, customizing them for specific tasks and domains.\\n  * **Pipeline Creation:** It facilitates the construction of AI pipelines, allowing users to chain together multiple models and components to perform complex workflows.\\n  * **Deployment:** Langsmith provides tools for deploying trained models as APIs, making them readily available for integration into applications.\\n  * **Open Source:** Being open-source, Langsmith encourages community collaboration, contributions, and transparency in AI development.\\n\\n  Langsmith empowers developers and businesses to leverage the potential of AI without requiring extensive technical expertise. Its intuitive platform and versatile features make it a valuable resource for building innovative NLU-driven applications.'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-source platform for developing and deploying large language models (LLMs).</description>\\n  <features>\\n    <feature>Modular and extensible design</feature>\\n    <feature>Supports various LLM architectures</feature>\\n    <feature>Provides tools for fine-tuning and evaluating LLMs</feature>\\n    <feature>Offers a user-friendly interface for interacting with LLMs</feature>\\n  </features>\\n  <benefits>\\n    <benefit>Accelerates LLM development and deployment</benefit>\\n    <benefit>Enables customization and experimentation</benefit>\\n    <benefit>Promotes collaboration and knowledge sharing</benefit>\\n  </benefits>\\n  <website>https://github.com/langsmith</website>\\n</response>  \\n``` \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 195, 'total_tokens': 388, 'completion_time': 0.350909091, 'prompt_time': 0.0088981, 'queue_time': 0.257947341, 'total_time': 0.359807191}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--41f0210b-5164-495a-96f3-8b061ba3ac86-0' usage_metadata={'input_tokens': 195, 'output_tokens': 193, 'total_tokens': 388}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a modular and customizable toolkit for tasks like chaining together different LLMs, connecting them to external data sources, and managing their interactions. Think of it as a way to build sophisticated AI applications using the power of LLMs in a more structured and efficient manner.</answer></response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 39, 'total_tokens': 129, 'completion_time': 0.163636364, 'prompt_time': 0.00248107, 'queue_time': 0.244028152, 'total_time': 0.166117434}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--e31e5013-0101-49a2-b3b6-dad3846ee4b3-0' usage_metadata={'input_tokens': 39, 'output_tokens': 90, 'total_tokens': 129}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movie>Big</movie>\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>Captain Phillips</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field!')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2999f98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d0bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_2_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
